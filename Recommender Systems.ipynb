{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24d3878",
   "metadata": {},
   "source": [
    "# Recommender Systems - Data Mining Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb35d5",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c58164",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'data/lab2_train.csv' \n",
    "test_file_path = 'data/lab2_test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_file_path, delimiter=',')\n",
    "test_data = pd.read_csv(test_file_path, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac05751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "like_train_data = train_data[train_data.is_like == True]\n",
    "match_train_data = train_data[train_data.is_match == True]\n",
    "dislike_train_data = train_data[train_data.is_like == False]\n",
    "\n",
    "liked_user_count = like_train_data[\"user_to_id\"].nunique()\n",
    "user_count = like_train_data[\"user_from_id\"].nunique()\n",
    "match_count = match_train_data[\"user_from_id\"].nunique()\n",
    "\n",
    "number_given_likes = like_train_data[\"user_from_id\"]\n",
    "likes_given_counts = number_given_likes.value_counts()\n",
    "dislikes_given_counts = dislike_train_data[\"user_from_id\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(likes_given_counts.index, likes_given_counts.values, color='blue')\n",
    "\n",
    "plt.xlabel('User IDs')\n",
    "plt.ylabel('Number of given Likes')\n",
    "plt.title('Distribution of Number of given Likes per User ID')\n",
    "plt.xticks([])  # Hides x axis labels due to the large number of ids\n",
    "plt.show()\n",
    "\n",
    "number_of_likes = like_train_data[\"user_to_id\"]\n",
    "received_counts = number_of_likes.value_counts()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(received_counts.index, received_counts.values, color='blue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c63ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_and_received_likes_per_id = pd.concat([likes_given_counts, received_counts], axis=1)\n",
    "given_and_received_likes_per_id.fillna(0, inplace=True)\n",
    "given_and_received_likes_per_id.columns = [\"given likes\", \"received likes\"]\n",
    "\n",
    "given_and_received_dislikes_per_id = pd.concat([dislikes_given_counts, received_counts], axis=1)\n",
    "given_and_received_dislikes_per_id.fillna(0, inplace=True)\n",
    "given_and_received_dislikes_per_id.columns = [\"given dislikes\", \"received likes\"]\n",
    "\n",
    "given_and_received_likes_per_id.plot.scatter(x=\"given likes\", y=\"received likes\", alpha=0.5, figsize=(15,8))\n",
    "\n",
    "plt.xlabel(\"number of given likes\")\n",
    "plt.ylabel(\"number of received likes\")\n",
    "plt.title(\"Number of given likes vs number of received likes\")\n",
    "plt.show()\n",
    "\n",
    "given_and_received_dislikes_per_id.plot.scatter(x=\"given dislikes\", y=\"received likes\", alpha=0.5, figsize=(15,8))\n",
    "\n",
    "plt.xlabel(\"number of given dislikes\")\n",
    "plt.ylabel(\"number of received likes\")\n",
    "plt.title(\"Number of given dislikes vs Number of received likes\")\n",
    "plt.show()\n",
    "\n",
    "activity = likes_given_counts.add(dislikes_given_counts, fill_value=0)\n",
    "activity_per_id = pd.concat([activity, received_counts], axis=1)\n",
    "activity_per_id.fillna(0, inplace=True)\n",
    "activity_per_id.columns = [\"activity\", \"received likes\"]\n",
    "\n",
    "activity_per_id.plot.scatter(x= \"activity\", y=\"received likes\", alpha=0.5, figsize=(15,8))\n",
    "plt.xlabel(\"number of given likes or dislikes\")\n",
    "plt.ylabel(\"number of received likes\")\n",
    "plt.title(\"Activity of user vs Number of received likes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235d2e8",
   "metadata": {},
   "source": [
    "## Familiarization\n",
    "### Important properties of the data\n",
    "There are only 1707 user who have one like or more of the 2107 users who gave likes. Of all of these users there are only 415 matches. Of these matches there are 345 unique users who receive a match. So there is only a small group of users who receive more than one match. And a small number of matches on the total of 76392 likes and dislikes.\n",
    "\n",
    "The Data is therefore very sparse. Of these 76392 likes and dislikes there are only 12637 likes. This is 16,5% likes. Of these likes there are only 415 matches. Thus only 3,3% mutual likes. This is sparse data to work with. You see in the visualization that there is some correlation between the number of received likes and the number of given likes. You see in the visualization above that the more likes a person received correlates with the less likes a user gives. Since that user gets a lot of likes the user gets critical for liking another user. Thus it also holds for when a user doesn't receive many likes than that user has is less critical of other users and gives more likes.\n",
    "\n",
    "\n",
    "### Types of people\n",
    "\n",
    "- Someone who receives a lot of likes but likes almost no one. User 2 receives 45 likes but likes none of them. Therefore not leading to a match\n",
    "\n",
    "- Someone who receives and gives about only dislikes. For example user 28 is very active on the platform but gives about only dislikes and receives only dislikes. Therefore not leading to a match\n",
    "\n",
    "- Someone who gives a lot of likes and receives only dislikes. For example user 18 gives 26 likes and receives no likes back and has thus not a match.\n",
    "\n",
    "- Someone who receives a lot of likes and gives a lot of likes. For example user 36 gives a lot of likes and receives a lot of likes and eventualy leading to a match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece31938",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = train_data[\"user_from_id\"].value_counts()\n",
    "\n",
    "valid_users = user_data[user_data >= 5].index\n",
    "train_data = train_data[train_data[\"user_from_id\"].isin(valid_users)]\n",
    "\n",
    "train_data = train_data.drop_duplicates()\n",
    "\n",
    "def nmf(X, mask, hyper_param, n_components: int, max_iter: int=1000, tol: float=1e-3):\n",
    "  \"\"\"\n",
    "  Decomposes the original sparse matrix X into two matrices W and H. \n",
    "  \"\"\"\n",
    "  # Initialize W and H with random non-negative values\n",
    "  W = np.random.rand(X.shape[0], n_components)\n",
    "  H = np.random.rand(n_components, X.shape[1])\n",
    "\n",
    "  E = np.sum(((X - W @ H)**2) * mask)\n",
    "  newE = 0.\n",
    "  i = 0\n",
    "\n",
    "  x_masked = X * mask\n",
    "  while E - newE > tol and i < max_iter: \n",
    "    E = np.sum(((X - W @ H)**2) * mask)\n",
    "\n",
    "    nominatorH = W.T @ x_masked\n",
    "    denominatorH = (W.T @ ((W @ H) * mask) + 1e-9)\n",
    "    H = H * nominatorH / denominatorH\n",
    "\n",
    "    nominatorW = (x_masked @ H.T)\n",
    "    denominatorW = (((W @ H) * mask) @ H.T + 1e-9)\n",
    "    W = W * nominatorW / denominatorW\n",
    "\n",
    "    newE = np.sum(((X - W @ H)**2) * mask)\n",
    "    i += 1\n",
    "\n",
    "  print(E)\n",
    "  print(i)\n",
    "  return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085eaf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.to_numpy()[:,[0,1,3]].astype(float)\n",
    "y = train_data.to_numpy()[:, 2]\n",
    "y = np.where(y == False, 0., 1.)\n",
    "x[:,2] = np.where(x[:,2] == False, 0., 1.)\n",
    "x[:,:2] = x[:,:2] / 5000.\n",
    "y = y.astype(float)\n",
    "x = x.astype(float)\n",
    "\n",
    "\n",
    "def split_data():\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.001, random_state=42)\n",
    "\n",
    "  mask = np.ones((len(x), 4))\n",
    "  mask[len(x_train):, 3:] = 0\n",
    "\n",
    "  train_matrix = np.hstack((np.vstack((x_train, x_test)), np.vstack((y_train.reshape(-1, 1), y_test.reshape(-1, 1)))))\n",
    "  return train_matrix, y_train, mask, len(x_train)\n",
    "\n",
    "def eval(X, M, components, train_length):\n",
    "  ys = []\n",
    "\n",
    "  W, H = nmf(X, M, 100., components)\n",
    "  predicted_matrix = W @ H\n",
    "  return predicted_matrix[:train_length, 3:]\n",
    "\n",
    "def calculate():\n",
    "  for j in range(35, 36, 1):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    train_matrix, y_test, mask, train_length = split_data()\n",
    "    results_gotten = eval(train_matrix, mask, j, train_length)\n",
    "    results_actual = y_test\n",
    "    plt.scatter(results_actual, results_gotten, label=f\"{j} components\", alpha=0.5)\n",
    "    plt.xlabel(\"real score\")\n",
    "    plt.ylabel(\"predicted score\")\n",
    "    plt.title(\"real score vs predicted score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "calculate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b14e5",
   "metadata": {},
   "source": [
    "\n",
    "## Mask\n",
    "As can be seen in the nmf method, we make use of masked_x to improve the speed. And we perform element-wise multiplication in the error function and the cumulative update function.\n",
    "\n",
    "## Components\n",
    "We have done extensive testing, and we have seen the best performance for 35 components. This variable can be changed in the code, to compare the results. The pre exisiting range can be filled in for more ease during visualization.\n",
    "\n",
    "## Pre-processing\n",
    "We have removed all users that have liked less than 5 users, or have less than 5 likes. Additionally, we remove the duplicates from the dataframe.\n",
    "\n",
    "## Normalization\n",
    "Since the features(id's), have significantly larger values than the classification values. The range of 0-3500 compared to 0-1. We therefore chose to scale the feature value down by 5000.\n",
    "\n",
    "## Recommendation threshold\n",
    "Since it seems like the values are located at 0 & 1, we want to place our threshold at 0.5 for maximum margin.\n",
    "\n",
    "Using this pipeline, we have first split the input data into a train and verification dataset. We then train this data by use of a mask. We then plot the data that we have masked out, and see that we can correctly predict the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d73c59",
   "metadata": {},
   "source": [
    "# Distance-based pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "# pre-processing the training data\n",
    "def create_binary_matrix(train_data):\n",
    "    user_from_ids = train_data['user_from_id'].unique()\n",
    "    user_to_ids = train_data['user_to_id'].unique()\n",
    "    matrix = np.zeros((len(user_from_ids), len(user_to_ids)), dtype=int)\n",
    "\n",
    "    for _, row in train_data.iterrows():\n",
    "        user_from_index = np.where(user_from_ids == row['user_from_id'])[0][0]\n",
    "        user_to_index = np.where(user_to_ids == row['user_to_id'])[0][0]\n",
    "        # Assigning weights: 2 for match, 1 for like\n",
    "        matrix[user_from_index, user_to_index] = 2 if row['is_match'] else (1 if row['is_like'] else 0)\n",
    "\n",
    "\n",
    "    return csr_matrix(matrix), user_from_ids, user_to_ids\n",
    "\n",
    "matrix, user_from_ids, user_to_ids = create_binary_matrix(train_data)\n",
    "\n",
    "# Using the Min Hash function to get the signatures using random hash values\n",
    "def min_hash(matrix, num_hash_functions=4):\n",
    "    num_rows, num_cols = matrix.shape\n",
    "    signatures = np.full((num_hash_functions, num_cols), np.inf)\n",
    "\n",
    "    for r in range(num_rows):\n",
    "        hash_values = np.random.permutation(num_rows)[:num_hash_functions]\n",
    "        for c in matrix[r].nonzero()[1]:\n",
    "            signatures[:, c] = np.minimum(signatures[:, c], hash_values)\n",
    "\n",
    "    return signatures\n",
    "\n",
    "signatures = min_hash(matrix)\n",
    "\n",
    "# Finding the similarity in signatures using Jaccard distance\n",
    "def jaccard_similarity(sig_col1, sig_col2):\n",
    "    return np.sum(sig_col1 == sig_col2) / len(sig_col1)\n",
    "\n",
    "# Find nearest neighbors for a user_id\n",
    "def nearest_neighbors(signatures, target_col, k=5):\n",
    "    similarities = []\n",
    "    for i in range(signatures.shape[1]):\n",
    "        if i != target_col:\n",
    "            similarity = jaccard_similarity(signatures[:, target_col], signatures[:, i])\n",
    "            similarities.append((i, similarity))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "# Find nearest neighbors for a user_id of choice\n",
    "neighbors = nearest_neighbors(signatures, 36)\n",
    "print(\"Nearest neighbors:\", neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab55503",
   "metadata": {},
   "source": [
    "# Report on Distance based Recommender System\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Data transformation\n",
    "- **Input Data**: The raw dataset consists of user interactions, including the data values `user_from_id`, `user_to_id`, `is_like`, and `is_match`.\n",
    "- **Binary Matrix Creation**: The data is transformed into a binary matrix. Rows represent users, and columns represent items. Cells in the matrix indicate the presence (1) or absence (0) of an interaction.\n",
    "\n",
    "### Weighting scheme (Enhanced Preprocessing)\n",
    "- **Importance Differentiation**: The system differentiates between 'like' and 'match'. 'Match' is given higher importance with a weight of 2, and 'like' is assigned a weight of 1.\n",
    "- **Matrix Representation**: This weighting scheme is incorporated into the binary matrix, enhancing the traditional binary representation to reflect the strength of interactions.\n",
    "\n",
    "## Min Hashing\n",
    "\n",
    "- **Hash Function Generation**: A set of hash functions is generated. The number of functions (k) is a tunable parameter. After tuning we chose to use here for 4 randomized hash functions. To give a balanced result.\n",
    "- **Signature Matrix Creation**: Each user is hashed using these functions to create a min-hash signature, reducing dimensionality while preserving similarity.\n",
    "\n",
    "## Nearest neighbor search\n",
    "\n",
    "- **Jaccard Distance Calculation**: The Jaccard distance between min hash signatures is used to quantify similarity between the users.\n",
    "- **Nearest Neighbors Identification**: For each user the system identifies the nearest neighbors. These are the users with the most similar min-hash signatures. We have chosen to return the top 5 of these nearest neighboars. In the case of Breeze these 5 users can be shown on a certain day. 5 is a feasable and not overwhelming number. Thus keeping the goals and process of the app intact.\n",
    "\n",
    "## Post-processing\n",
    "\n",
    "- **Aggregation for Prediction**: For a new user-item pair, the system estimates the likelihood of interaction (like or match) based on the information aggregated from nearest neighbors.\n",
    "- **Recommendation Criteria**: Recommendations are based on the interactions of nearest neighbors, prioritizing items with higher aggregated scores.\n",
    "- **Result Delivery**: The system outputs a list of the top five recommended users for each user, sorted by the likelihood of a like and match.\n",
    "\n",
    "## Analyse result\n",
    "\n",
    "You see in the result of the distance based recommender system when using it that there are users who receive neighbours with high likelihood and there are users who receive neighbours with lower likelihood. This is a normal result for this kind of program. Because this depends on various criteria. If a certain user doesn't get any likes and doesn't give any likes. Their likelihood of liking a certain neigbour and receiving a match is very low. This you see back in the result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fe17e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
